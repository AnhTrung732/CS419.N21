{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykZPJGLIqzHt",
        "outputId": "9eebaae5-9768-4876-a8d7-5c308c809ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ir_datasets\n",
            "  Downloading ir_datasets-0.5.4-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.5/311.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting warc3-wet-clueweb09>=0.2.5\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (6.0)\n",
            "Collecting pyautocorpus>=0.1.1\n",
            "  Downloading pyautocorpus-0.1.9-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (4.9.2)\n",
            "Collecting lz4>=3.1.1\n",
            "  Downloading lz4-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (4.11.2)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (1.22.4)\n",
            "Collecting zlib-state>=0.1.3\n",
            "  Downloading zlib_state-0.1.5-cp39-cp39-manylinux2010_x86_64.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unlzw3>=0.2.1\n",
            "  Downloading unlzw3-0.2.2-py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (2.27.1)\n",
            "Collecting trec-car-tools>=2.5.4\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Collecting warc3-wet>=0.2.3\n",
            "  Downloading warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from ir_datasets) (4.65.0)\n",
            "Collecting ijson>=3.1.3\n",
            "  Downloading ijson-3.2.0.post0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->ir_datasets) (2022.12.7)\n",
            "Collecting cbor>=1.0.0\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18918 sha256=e166e1a70eb231d3cc948446598a0b8328a90aa005f99be3fba8f540be91d3e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/22/ed/a11944d7fdf4e94c4206a3f760d385122a4d34d8acc12f71a3\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp39-cp39-linux_x86_64.whl size=57288 sha256=eee37c7923886939668106dd09dac093d6c4fa9ea0f4abd868539d69f1907fa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/10/03/a281e0682ddd4b310431fb25d1a4f53987105267cf46c417f3\n",
            "Successfully built warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, trec-car-tools, pyautocorpus, lz4, ir_datasets\n",
            "Successfully installed cbor-1.0.0 ijson-3.2.0.post0 ir_datasets-0.5.4 lz4-4.3.2 pyautocorpus-0.1.9 trec-car-tools-2.6 unlzw3-0.2.2 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install ir_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q6K53AWrCPh",
        "outputId": "f83cd039-9ee2-45c1-d1bb-78a5f89959d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ir_datasets\n",
        "dataset = ir_datasets.load(\"cranfield\")"
      ],
      "metadata": {
        "id": "w-IDwdI9rSw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [doc.text for doc in dataset.docs_iter()]"
      ],
      "metadata": {
        "id": "l-Q9-ZzVrOqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ed7595-ae83-41ba-9a86-1b6ba838bc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO] [starting] http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz\n",
            "[INFO] [finished] http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz: [00:00] [507kB] [962kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz-g_qPjr2zE",
        "outputId": "3ad8cc17-f81d-407c-a497-3bd3b1a27a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['experimental investigation of the aerodynamics of a\\nwing in a slipstream .\\n  an experimental study of a wing in a propeller slipstream was\\nmade in order to determine the spanwise distribution of the lift\\nincrease due to slipstream at different angles of attack of the wing\\nand at different free stream to slipstream velocity ratios .  the\\nresults were intended in part as an evaluation basis for different\\ntheoretical treatments of this problem .\\n  the comparative span loading curves, together with\\nsupporting evidence, showed that a substantial part of the lift increment\\nproduced by the slipstream was due to a /destalling/ or\\nboundary-layer-control effect .  the integrated remaining lift\\nincrement, after subtracting this destalling lift, was found to agree\\nwell with a potential flow theory .\\n  an empirical evaluation of the destalling effects was made for\\nthe specific configuration of the experiment .',\n",
              " \"simple shear flow past a flat plate in an incompressible fluid of small\\nviscosity .\\nin the study of high-speed viscous flow past a two-dimensional body it\\nis usually necessary to consider a curved shock wave emitting from the\\nnose or leading edge of the body .  consequently, there exists an\\ninviscid rotational flow region between the shock wave and the boundary\\nlayer .  such a situation arises, for instance, in the study of the\\nhypersonic viscous flow past a flat plate .  the situation is somewhat\\ndifferent from prandtl's classical boundary-layer problem . in prandtl's\\noriginal problem the inviscid free stream outside the boundary layer is\\nirrotational while in a hypersonic boundary-layer problem the inviscid\\nfree stream must be considered as rotational .  the possible effects of\\nvorticity have been recently discussed by ferri and libby .  in the\\npresent paper, the simple shear flow past a flat plate in a fluid of small\\nviscosity is investigated .  it can be shown that this problem can again\\nbe treated by the boundary-layer approximation, the only novel feature\\nbeing that the free stream has a constant vorticity .  the discussion\\nhere is restricted to two-dimensional incompressible steady flow .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLryWvn5sJ-c",
        "outputId": "2b86e474-bab6-428f-a7e7-d5dca659fbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prepare data "
      ],
      "metadata": {
        "id": "wYJvfQCfjNkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "preprocessed_corpus = []\n",
        "for doc in corpus:\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    preprocessed_corpus.append(' '.join(filtered_tokens))"
      ],
      "metadata": {
        "id": "txUbEAP8q-5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [query.text for query in dataset.queries_iter()]\n",
        "rels = {}\n",
        "for qrel in dataset.qrels_iter():\n",
        "  rels[int(qrel.query_id)] = []\n",
        "for qrel in dataset.qrels_iter():\n",
        "  rels[int(qrel.query_id)].append(int(qrel.doc_id))"
      ],
      "metadata": {
        "id": "yQcGgSz-jMYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_doc_tfidf(docs):\n",
        "  appear_in_docs = {}\n",
        "  posting_list = {}\n",
        "\n",
        "  # init \n",
        "  for doc in docs:\n",
        "    for term in doc.split():\n",
        "      appear_in_docs[term] = 0\n",
        "      posting_list[term] = []\n",
        "\n",
        "\n",
        "  for i, doc in enumerate(docs):\n",
        "    appear_in_this = {}\n",
        "    for term in doc.split():\n",
        "      if not term in appear_in_this:\n",
        "        appear_in_docs[term] += 1\n",
        "        posting_list[term].append(i)\n",
        "\n",
        "      appear_in_this[term] = 1\n",
        "\n",
        "  idf = {}\n",
        "  for term in posting_list:\n",
        "    idf[term] = math.log2(1400 / appear_in_docs[term])\n",
        "\n",
        "  docs_weight = []\n",
        "  for doc in docs:\n",
        "    cur_weight = {}\n",
        "    tf = {}\n",
        "    for term in doc.split():\n",
        "      tf[term] = 0\n",
        "    for term in doc.split():\n",
        "      tf[term] += 1\n",
        "\n",
        "    for term in tf:\n",
        "      cur_weight[term] = tf[term] * idf[term]\n",
        "    \n",
        "    docs_weight.append(cur_weight)\n",
        "\n",
        "  return docs_weight, posting_list, idf"
      ],
      "metadata": {
        "id": "BedAiLbPX-4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_tfidf, posting, idf = build_doc_tfidf(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "hIRiYfRWalHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# query"
      ],
      "metadata": {
        "id": "5-Cp18nDjUtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = {}\n",
        "for i, query in tqdm(enumerate(queries)): \n",
        "\n",
        "  tokens = word_tokenize(query.lower())\n",
        "  terms = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "  # calc weights for query\n",
        "  qtf = {} \n",
        "  for term in terms:\n",
        "    qtf[term] = 0\n",
        "  for term in terms:\n",
        "    qtf[term] += 1\n",
        "  \n",
        "  myDocScore = {}\n",
        "  for term in terms:\n",
        "    if term in posting:\n",
        "      for doc_id in posting[term]:\n",
        "        if doc_id not in myDocScore:\n",
        "          myDocScore[doc_id] = doc_tfidf[doc_id][term] * (qtf[term] * idf[term])\n",
        "        else:\n",
        "          myDocScore[doc_id] += doc_tfidf[doc_id][term] * (qtf[term] * idf[term])\n",
        "\n",
        "  # divide by len because the weight is not normalized yet\n",
        "  for id in myDocScore:\n",
        "    myDocScore[id] /= len(doc_tfidf[id])\n",
        "    \n",
        "  sorted_score = [key+1 for key, value in sorted(myDocScore.items(), key=lambda item: -item[1])]\n",
        "  res[i+1] = sorted_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yD0OaqnsxEn",
        "outputId": "94f42bf5-da37-4554-d5c2-28bb48a4ac8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "225it [00:00, 288.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate"
      ],
      "metadata": {
        "id": "k_dB7SQBjWwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_interpolated_map(queries, relevant_docs):\n",
        "    sum=0\n",
        "    lens=len(queries.keys())\n",
        "    for query in queries.keys():\n",
        "        ranked_docs = queries[query]\n",
        "        precision = []\n",
        "        recall = []\n",
        "        relevant = set(relevant_docs[query])\n",
        "        retrieved = set()\n",
        "        for i, doc in enumerate(ranked_docs):\n",
        "            if doc in relevant:\n",
        "                retrieved.add(doc)\n",
        "            precision.append(len(retrieved) / (i + 1))\n",
        "            recall.append(len(retrieved) / len(relevant))\n",
        "        precision_sum=0\n",
        "        for i in range(11):\n",
        "            recall_level=i/10\n",
        "            precision_list=[precision[i] for i in range(len(precision)) if recall[i] >= recall_level]\n",
        "            if(len(precision_list)==0):\n",
        "                precision_list=0\n",
        "            else:\n",
        "              precision_list=max(precision_list)\n",
        "            precision_sum+=(precision_list/11)\n",
        "        sum+=(precision_sum/lens)\n",
        "\n",
        "    return sum"
      ],
      "metadata": {
        "id": "YXvlnw0xxrWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_vector_score = res\n",
        "true_answer = rels"
      ],
      "metadata": {
        "id": "qTtU6ZOEzhpx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_interpolated_map(result_vector_score, true_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Jo5kElv3nL",
        "outputId": "1b11161e-7099-4678-a55a-a957f5ea2428"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3436494952901474"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}